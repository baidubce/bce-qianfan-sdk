{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 qianfan sdk 构建本地自动评估模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "千帆大模型平台提供了在线进行自动评估的方式，而有时我们希望有更灵活的自动评估方式。\n",
    "\n",
    "本文将以评估文本摘要效果为例子来介绍如何使用千帆 sdk 自定义自动评估模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处使用0.2.7版本以上的qianfan sdk。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:33.543823Z",
     "start_time": "2024-01-17T09:05:33.534752Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U \"qianfan>=0.2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行鉴权认证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T03:11:13.789423Z",
     "start_time": "2024-01-18T03:11:13.739487Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['QIANFAN_ACCESS_KEY'] = 'your_access_key'\n",
    "os.environ['QIANFAN_SECRET_KEY'] = 'your_secret_key'\n",
    "\n",
    "# 进行评估时，请按实际情况填写 QPS LIMIT, 否则会报错\n",
    "os.environ[\"QIANFAN_QPS_LIMIT\"] = \"1\"\n",
    "os.environ['QIANFAN_LLM_API_RETRY_COUNT'] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先构造待评估数据集，此处用本地数据集，也可以拉取千帆平台上的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.433207Z",
     "start_time": "2024-01-17T09:05:33.552097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。', 'response': [['修改后的立法法全文公布']], '_group': 0}\n"
     ]
    }
   ],
   "source": [
    "from qianfan.dataset import Dataset\n",
    "\n",
    "ds = Dataset.load(data_file=\"data_summerize/excerpt.jsonl\", organize_data_as_group=True)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后创建prompt模版，用于引导模型进行评估。\n",
    "\n",
    "由于摘要任务没有标准答案，所以此处模板不提供，如果是评估问答任务，则需要提供标准答案以评估正确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.439700Z",
     "start_time": "2024-01-17T09:05:34.433919Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"\n",
    "你是一名裁判员，负责为给定新闻的摘要评分。\n",
    "\n",
    "评价标准：\n",
    "\n",
    "{criteria}\n",
    "\n",
    "请你遵照以下的评分步骤：\n",
    "{steps}\n",
    "\n",
    "\n",
    "例子：\n",
    "\n",
    "新闻：\n",
    "\n",
    "{prompt}\n",
    "\n",
    "摘要：\n",
    "\n",
    "{response}\n",
    "\n",
    "\n",
    "根据答案的综合水平给出0到{max_score}之间的整数评分。\n",
    "如果答案存在明显的不合理之处，则应给出一个较低的评分。\n",
    "如果答案符合以上要求并且与参考答案含义相似，则应给出一个较高的评分\n",
    "\n",
    "你的回答模版如下(只输出整数评分)：\n",
    "\n",
    "{metric_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着指定评估指标和评估步骤。\n",
    "\n",
    "此处从四个维度进行评估，分别是相关性、连贯性、一致性、流畅度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.440367Z",
     "start_time": "2024-01-17T09:05:34.438126Z"
    }
   },
   "outputs": [],
   "source": [
    "relevance_metric = \"\"\"\n",
    "相关性 - 摘要中重要内容的选择。\\ \n",
    "摘要只应包含来自源文档的重要信息。 \\\n",
    "惩罚包含冗余和多余信息的摘要。\n",
    "\"\"\"\n",
    "\n",
    "relevance_steps = \"\"\"\n",
    "1. 仔细阅读摘要和源文档。\n",
    "2. 将摘要与源文档进行比较，并识别文章的主要观点。\n",
    "3. 评估摘要是否涵盖了文章的主要观点，以及它包含多少无关或冗余信息。\n",
    "\"\"\"\n",
    "\n",
    "relevance_max_score = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连贯性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.448399Z",
     "start_time": "2024-01-17T09:05:34.440976Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "coherence_metric = \"\"\"\n",
    "连贯性 - 所有句子的总体质量。 \\\n",
    "我们将此维度与 DUC 质量问题结构性和连贯性相关， \\\n",
    "其中“摘要应具有良好的结构和组织，不应只是相关信息的堆叠，而应基于从句子到主题有关的信息的连贯性主体进行构建。” \\\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "coherence_steps = \"\"\"\n",
    "1. 仔细阅读文章并识别主要主题和关键点。\n",
    "2. 阅读摘要并与文章进行比较。检查摘要是否涵盖了文章的主要主题和关键点，并以清晰且逻辑的顺序呈现它们\n",
    "\"\"\"\n",
    "\n",
    "coherence_max_score = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.448902Z",
     "start_time": "2024-01-17T09:05:34.444506Z"
    }
   },
   "outputs": [],
   "source": [
    "consistency_metric = \"\"\"\n",
    "一致性 - 摘要与摘要源的客观对齐。 \\\n",
    "客观一致的摘要只包含与源文档支持的陈述。 \\\n",
    "惩罚包含幻觉事实的摘要。\n",
    "\"\"\"\n",
    "\n",
    "consistency_steps = \"\"\"\n",
    "1. 仔细阅读文章并识别主要事实和细节。\n",
    "2. 阅读摘要并与文章进行比较。检查摘要是否包含任何不支持的文章的事实错误。\n",
    "3. 基于评估标准，为一致性分配分数。\n",
    "\"\"\"\n",
    "\n",
    "consistency_max_score = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流畅度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.455710Z",
     "start_time": "2024-01-17T09:05:34.447553Z"
    }
   },
   "outputs": [],
   "source": [
    "fluency_metric = \"\"\"\n",
    "流畅度：摘要的语法、拼写、标点、单词选择和句子结构的质量。\n",
    "1：差。摘要有很多错误，使它难以理解或听起来不自然。\n",
    "2：中。摘要有一些影响文本清晰度或流畅性的错误，但要点仍然可理解。\n",
    "3：好。摘要很少或没有错误，易于阅读和遵循。\n",
    "\"\"\"\n",
    "\n",
    "fluency_steps = \"\"\"\n",
    "读取摘要并基于给定的标准评估其流畅度。从 1 到 3 分配一个流畅度分数。\n",
    "\"\"\"\n",
    "\n",
    "fluency_max_score = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用ChatCompletion模型构造评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.456387Z",
     "start_time": "2024-01-17T09:05:34.451199Z"
    }
   },
   "outputs": [],
   "source": [
    "import qianfan\n",
    "import time\n",
    "\n",
    "def get_geval_score(chat_comp,**kwargs):\n",
    "    prompt = evaluation_prompt_template.format(**kwargs)\n",
    "    msg = qianfan.Messages()\n",
    "    msg.append(prompt, role='user')\n",
    "    resp = chat_comp.do(\n",
    "        messages=msg,\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "    )\n",
    "    return resp['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后遍历数据集进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:34.477888Z",
     "start_time": "2024-01-17T09:05:34.454501Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation_metrics = {\n",
    "    \"Relevance\": (relevance_metric, relevance_metric, relevance_max_score),\n",
    "    \"Coherence\": (coherence_metric, coherence_steps, coherence_max_score),\n",
    "    \"Consistency\": (consistency_metric, consistency_steps, consistency_max_score),\n",
    "    \"Fluency\": (fluency_metric, fluency_steps, fluency_max_score),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:48.948712Z",
     "start_time": "2024-01-17T09:05:34.461234Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_comp = qianfan.ChatCompletion(model=\"ERNIE-Bot-4\")\n",
    "result = {\"Evaluation Type\": [], \"News id\": [], \"Score\": []}\n",
    "for eval_type, (criteria, steps, max_score) in evaluation_metrics.items():\n",
    "    for ind, data in enumerate(ds):\n",
    "        result[\"Evaluation Type\"].append(eval_type)\n",
    "        result[\"News id\"].append(ind)\n",
    "        evaluate = get_geval_score(\n",
    "            chat_comp,\n",
    "            criteria=criteria,\n",
    "            steps=steps,\n",
    "            prompt=data['prompt'],\n",
    "            response=data['response'],\n",
    "            max_score=max_score,\n",
    "            metric_name=eval_type      \n",
    "        )\n",
    "        score = int(evaluate.strip())\n",
    "        result[\"Score\"].append(evaluate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:05:48.975708Z",
     "start_time": "2024-01-17T09:05:48.955062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluation Type  Coherence  Consistency  Fluency  Relevance  Mean_Score\nNews id                                                                \n0                        8            8        2          8         6.5\n1                        8            8        2          8         6.5\n2                        5            5        2          6         4.5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Evaluation Type</th>\n      <th>Coherence</th>\n      <th>Consistency</th>\n      <th>Fluency</th>\n      <th>Relevance</th>\n      <th>Mean_Score</th>\n    </tr>\n    <tr>\n      <th>News id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>8</td>\n      <td>2</td>\n      <td>8</td>\n      <td>6.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>8</td>\n      <td>2</td>\n      <td>8</td>\n      <td>6.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>6</td>\n      <td>4.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pivot_df = pd.DataFrame(result, index=None).pivot(\n",
    "    columns=\"Evaluation Type\", index=\"News id\", values=\"Score\"\n",
    ").astype(int)\n",
    "pivot_df['Mean_Score'] = pivot_df.mean(axis=1)\n",
    "display(pivot_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
