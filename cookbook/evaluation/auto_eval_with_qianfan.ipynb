{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 qianfan sdk 构建本地评估模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "千帆大模型平台提供了在线进行自动评估的方式，而有时我们希望有更灵活的自动评估方式。\n",
    "\n",
    "本文将以评估文本摘要效果为例子来介绍如何使用千帆 sdk 自定义自动评估模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处使用0.2.8版本以上的qianfan sdk。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"qianfan>=0.2.8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行鉴权认证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:21.426489Z",
     "start_time": "2024-01-25T05:31:21.424399Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['QIANFAN_ACCESS_KEY'] = 'your_access_key'\n",
    "os.environ['QIANFAN_SECRET_KEY'] = 'your_secret_key'\n",
    "\n",
    "# 使用 Service 对象进行评估时，请按实际情况填写 QPS LIMIT，\n",
    "# 取值为所有 Service QPS Limit中的最小值\n",
    "os.environ[\"QIANFAN_QPS_LIMIT\"] = \"1\"\n",
    "os.environ['QIANFAN_LLM_API_RETRY_COUNT'] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据与模板准备\n",
    "\n",
    "首先构造待评估数据集，此处用本地数据集，也可以拉取千帆平台上的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.075507Z",
     "start_time": "2024-01-25T05:31:21.428111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。', 'response': [['修改后的立法法全文公布']], '_group': 0}\n"
     ]
    }
   ],
   "source": [
    "from qianfan.dataset import Dataset\n",
    "\n",
    "ds = Dataset.load(data_file=\"data_summerize/excerpt.jsonl\", organize_data_as_group=True, input_columns=[\"prompt\"], reference_column=\"response\")\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后创建prompt模版，用于引导模型进行评估。\n",
    "\n",
    "由于摘要任务没有标准答案，所以此处模板不提供，如果是评估问答任务，则需要提供标准答案以评估正确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.080048Z",
     "start_time": "2024-01-25T05:31:24.075411Z"
    }
   },
   "outputs": [],
   "source": [
    "from qianfan.common import Prompt\n",
    "\n",
    "evaluation_prompt_template = \"\"\"\n",
    "你是一名裁判员，负责为给定新闻的摘要评分。\n",
    "\n",
    "评价标准：\n",
    "\n",
    "{criteria}\n",
    "\n",
    "请你遵照以下的评分步骤：\n",
    "{steps}\n",
    "\n",
    "\n",
    "例子：\n",
    "\n",
    "新闻：\n",
    "\n",
    "{prompt}\n",
    "\n",
    "摘要：\n",
    "\n",
    "{response}\n",
    "\n",
    "\n",
    "根据答案的综合水平给出0到{max_score}之间的整数评分。\n",
    "如果答案存在明显的不合理之处，则应给出一个较低的评分。\n",
    "如果答案符合以上要求并且与参考答案含义相似，则应给出一个较高的评分\n",
    "\n",
    "你的回答模版只能输出整数评分,不输出文字\n",
    "\"\"\"\n",
    "\n",
    "evaluation_prompt = Prompt(\n",
    "    name=\"evaluation_prompt\",\n",
    "    template=evaluation_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着指定评估指标和评估步骤。\n",
    "\n",
    "此处从四个维度进行评估，分别是相关性、连贯性、一致性、流畅度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.080789Z",
     "start_time": "2024-01-25T05:31:24.078284Z"
    }
   },
   "outputs": [],
   "source": [
    "relevance_metric = \"\"\"\n",
    "相关性 - 摘要中重要内容的选择。 \\ \n",
    "摘要只应包含来自源文档的重要信息。 \\\n",
    "惩罚包含冗余和多余信息的摘要。\n",
    "\"\"\"\n",
    "\n",
    "relevance_steps = \"\"\"\n",
    "1. 仔细阅读摘要和源文档。\n",
    "2. 将摘要与源文档进行比较，并识别文章的主要观点。\n",
    "3. 评估摘要是否涵盖了文章的主要观点，以及它包含多少无关或冗余信息。\n",
    "\"\"\"\n",
    "\n",
    "relevance_max_score = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连贯性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.084380Z",
     "start_time": "2024-01-25T05:31:24.081503Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "coherence_metric = \"\"\"\n",
    "连贯性 - 所有句子的总体质量。 \\\n",
    "我们将此维度与 DUC 质量问题结构性和连贯性相关， \\\n",
    "其中“摘要应具有良好的结构和组织，不应只是相关信息的堆叠，而应基于从句子到主题有关的信息的连贯性主体进行构建。” \\\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "coherence_steps = \"\"\"\n",
    "1. 仔细阅读文章并识别主要主题和关键点。\n",
    "2. 阅读摘要并与文章进行比较。检查摘要是否涵盖了文章的主要主题和关键点，并以清晰且逻辑的顺序呈现它们\n",
    "\"\"\"\n",
    "\n",
    "coherence_max_score = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.087735Z",
     "start_time": "2024-01-25T05:31:24.085301Z"
    }
   },
   "outputs": [],
   "source": [
    "consistency_metric = \"\"\"\n",
    "一致性 - 摘要与摘要源的客观对齐。 \\\n",
    "客观一致的摘要只包含与源文档支持的陈述。 \\\n",
    "惩罚包含幻觉事实的摘要。\n",
    "\"\"\"\n",
    "\n",
    "consistency_steps = \"\"\"\n",
    "1. 仔细阅读文章并识别主要事实和细节。\n",
    "2. 阅读摘要并与文章进行比较。检查摘要是否包含任何不支持的文章的事实错误。\n",
    "3. 基于评估标准，为一致性分配分数。\n",
    "\"\"\"\n",
    "\n",
    "consistency_max_score = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流畅度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.091757Z",
     "start_time": "2024-01-25T05:31:24.088434Z"
    }
   },
   "outputs": [],
   "source": [
    "fluency_metric = \"\"\"\n",
    "流畅度：摘要的语法、拼写、标点、单词选择和句子结构的质量。\n",
    "1：差。摘要有很多错误，使它难以理解或听起来不自然。\n",
    "2：中。摘要有一些影响文本清晰度或流畅性的错误，但要点仍然可理解。\n",
    "3：好。摘要很少或没有错误，易于阅读和遵循。\n",
    "\"\"\"\n",
    "\n",
    "fluency_steps = \"\"\"\n",
    "读取摘要并基于给定的标准评估其流畅度。从 1 到 3 分配一个流畅度分数。\n",
    "\"\"\"\n",
    "\n",
    "fluency_max_score = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 封装评估函数\n",
    "\n",
    "用ChatCompletion模型构造评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.095523Z",
     "start_time": "2024-01-25T05:31:24.091843Z"
    }
   },
   "outputs": [],
   "source": [
    "import qianfan\n",
    "\n",
    "def get_geval_score(chat_comp, evaluation_prompt, **kwargs):\n",
    "    prompt, _ = evaluation_prompt.render(**kwargs)\n",
    "    msg = qianfan.Messages()\n",
    "    msg.append(prompt, role='user')\n",
    "    resp = chat_comp.do(\n",
    "        messages=msg,\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "    )\n",
    "    return resp['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后遍历数据集进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:24.129252Z",
     "start_time": "2024-01-25T05:31:24.094510Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation_metrics = {\n",
    "    \"Relevance\": (relevance_metric, relevance_metric, relevance_max_score),\n",
    "    \"Coherence\": (coherence_metric, coherence_steps, coherence_max_score),\n",
    "    \"Consistency\": (consistency_metric, consistency_steps, consistency_max_score),\n",
    "    \"Fluency\": (fluency_metric, fluency_steps, fluency_max_score),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:38.605514Z",
     "start_time": "2024-01-25T05:31:24.099352Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_comp = qianfan.ChatCompletion(model=\"ERNIE-Bot-4\")\n",
    "result = {\"Evaluation Type\": [], \"News id\": [], \"Score\": []}\n",
    "for eval_type, (criteria, steps, max_score) in evaluation_metrics.items():\n",
    "    for ind, data in enumerate(ds):\n",
    "        result[\"Evaluation Type\"].append(eval_type)\n",
    "        result[\"News id\"].append(ind)\n",
    "        evaluate = get_geval_score(\n",
    "            chat_comp,\n",
    "            evaluation_prompt,\n",
    "            criteria=criteria,\n",
    "            steps=steps,\n",
    "            prompt=data['prompt'],\n",
    "            response=data['response'][0][0],\n",
    "            max_score=str(max_score),\n",
    "            metric_name=eval_type      \n",
    "        )\n",
    "        score = int(evaluate.strip())\n",
    "        result[\"Score\"].append(evaluate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:38.641265Z",
     "start_time": "2024-01-25T05:31:38.613698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluation Type  Coherence  Consistency  Fluency  Relevance  Mean_Score\nNews id                                                                \n0                        8           10        3          9        7.50\n1                        8            8        2          8        6.50\n2                        5            5        2          5        4.25",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Evaluation Type</th>\n      <th>Coherence</th>\n      <th>Consistency</th>\n      <th>Fluency</th>\n      <th>Relevance</th>\n      <th>Mean_Score</th>\n    </tr>\n    <tr>\n      <th>News id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>10</td>\n      <td>3</td>\n      <td>9</td>\n      <td>7.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>8</td>\n      <td>2</td>\n      <td>8</td>\n      <td>6.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pivot_df = pd.DataFrame(result, index=None).pivot(\n",
    "    columns=\"Evaluation Type\", index=\"News id\", values=\"Score\"\n",
    ").astype(int)\n",
    "pivot_df['Mean_Score'] = pivot_df.mean(axis=1)\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 封装本地评估器\n",
    "\n",
    "除了上述进行本地评估的方法，也可以使用千帆平台提供的本地评估工具类进行评估\n",
    "\n",
    "首先继承LocalEvaluator类，并实现evaluate方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:38.652174Z",
     "start_time": "2024-01-25T05:31:38.637022Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "from qianfan.utils.pydantic import Field\n",
    "from qianfan.evaluation.consts import (\n",
    "    QianfanRefereeEvaluatorDefaultMaxScore,\n",
    "    QianfanRefereeEvaluatorDefaultMetrics,\n",
    "    QianfanRefereeEvaluatorDefaultSteps,\n",
    ")\n",
    "from qianfan.evaluation.evaluator import LocalEvaluator\n",
    "\n",
    "class QianfanLocalEvaluator(LocalEvaluator):\n",
    "\n",
    "    model: Any\n",
    "    metric_name: str = Field(default=\"\")\n",
    "    prompt_metrics: str = Field(default=QianfanRefereeEvaluatorDefaultMetrics)\n",
    "    prompt_steps: str = Field(default=QianfanRefereeEvaluatorDefaultSteps)\n",
    "    prompt_max_score: int = Field(default=QianfanRefereeEvaluatorDefaultMaxScore)\n",
    "\n",
    "    # evaluate三个参数，input是输入，reference是模型输出的结果，output是参考答案。\n",
    "    # 当evaluationManager调用eval时，如果is_chat_service=True，则input为对话列表 List[Dict[str, Any]]，否则为文本 str。\n",
    "    # 在该任务中，input是新闻，output是摘要，reference是service生成的摘要\n",
    "    # 由于我们只是对新闻摘要进行评分，因此只需要用到input和output\n",
    "    def evaluate(\n",
    "            self, input: Union[str, List[Dict[str, Any]]], reference: str, output: str\n",
    "        ) -> Dict[str, Any]:\n",
    "        # 将input调整为合适格式\n",
    "        input_content = input if isinstance(input, str) else input[0].get('content','')\n",
    "        # 生成评价模板\n",
    "        prompt, _ = evaluation_prompt.render(\n",
    "            criteria=self.prompt_metrics,\n",
    "            steps=self.prompt_steps,\n",
    "            prompt=input_content,\n",
    "            response=output,\n",
    "            max_score=str(self.prompt_max_score),\n",
    "            metric_name=self.metric_name,\n",
    "        )\n",
    "        # 调用模型获得评分\n",
    "        msg = qianfan.Messages()\n",
    "        msg.append(prompt)\n",
    "        resp = self.model.do(\n",
    "            messages=msg,\n",
    "            temperature=0.1,\n",
    "            top_p=1,\n",
    "        )\n",
    "        # print(f'{self.metric_name}|{input[0]}|{output}|{resp[\"result\"].strip()}')\n",
    "        return {self.metric_name: resp['result'].strip()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:38.656481Z",
     "start_time": "2024-01-25T05:31:38.653175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [01-25 13:31:38] model.py:375 [t:8607515264]: service type should be specified before exec\n"
     ]
    }
   ],
   "source": [
    "from qianfan.model import Service\n",
    "\n",
    "eb_turbo_service = Service(model=\"ERNIE-Bot-turbo\")  # 加载生成回答的服务\n",
    "chat_comp = qianfan.ChatCompletion(model=\"ERNIE-Bot-4\")  # 实例化用于裁判的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "附加本地评估器，供EvaluationManager调用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "local_evaluators = []\n",
    "for eval_type, (criteria, steps, max_score) in evaluation_metrics.items():\n",
    "    local_evaluator = QianfanLocalEvaluator(\n",
    "        prompt_metrics=criteria,\n",
    "        prompt_steps=steps,\n",
    "        prompt_max_score=max_score,\n",
    "        model=chat_comp,\n",
    "        metric_name=eval_type\n",
    "    )\n",
    "    local_evaluators.append(local_evaluator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:38.662843Z",
     "start_time": "2024-01-25T05:31:38.659903Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:55.052566Z",
     "start_time": "2024-01-25T05:31:38.661522Z"
    }
   },
   "outputs": [],
   "source": [
    "from qianfan.evaluation import EvaluationManager\n",
    "\n",
    "em = EvaluationManager(local_evaluators=local_evaluators)\n",
    "result = em.eval(\n",
    "    [eb_turbo_service], ds,\n",
    "    #  is_chat_service=False # 默认为True，此时input为List[Dict[str, Any]]，否则input为str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:55.067797Z",
     "start_time": "2024-01-25T05:31:55.057648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_chats': [{'content': '新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。', 'role': 'user'}], 'expected_output': '修改后的立法法全文公布', 'model_content': [{'Coherence': '8', 'Consistency': '10', 'Fluency': '3', 'Relevance': '9', 'content': '是的，您说的没错。新华社于2023年6月18日受权播发了修改后的《中华人民共和国立法法》。这次修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。\\n\\n这次修改的主要内容包括：完善全国人大及其常委会的立法程序和工作机制，落实党中央关于依法赋予设区的市立法权的要求，健全立法决策与行政决策相衔接机制，完善行政法规、地方性法规、自治条例和单行条例等的授权工作机制等。这些修改都是为了更好地适应当前社会发展和国家治理的需要，促进法治建设的发展。\\n\\n希望以上信息对您有所帮助，如果您还有其他问题，欢迎告诉我。', 'llm_tag': 'None_None_ERNIE-Bot-turbo'}]}\n",
      "{'input_chats': [{'content': '一辆小轿车，一名女司机，竟造成9死24伤。日前，深圳市交警局对事故进行通报：从目前证据看，事故系司机超速行驶且操作不当导致。目前24名伤员已有6名治愈出院，其余正接受治疗，预计事故赔偿费或超一千万元。', 'role': 'user'}], 'expected_output': '深圳机场9死24伤续：司机全责赔偿或超千万', 'model_content': [{'Coherence': '8', 'Consistency': '8', 'Fluency': '2', 'Relevance': '8', 'content': '这起事故是非常悲惨的，对受害者和家人造成了巨大的痛苦和损失。对于事故的原因，交警局的通报已经给出了明确的解释，即司机超速行驶且操作不当。这种行为是非常危险和不负责任的，不仅会危及自己的生命安全，也会对周围的人造成严重的伤害。\\n\\n目前，已经有6名伤员治愈出院，这表明医疗专业人员正在努力救治所有伤者，并且取得了积极的成果。同时，我们也要关注那些仍在接受治疗的人，希望他们能够尽快康复。\\n\\n对于事故的赔偿问题，预计赔偿费用可能超过一千万元，这是一个非常庞大的数字。这需要社会各界的支持和帮助，包括政府、保险公司、企业和个人等。应该采取各种措施，确保受害者和他们的家人得到充分的赔偿和支持，以帮助他们度过难关。\\n\\n总之，这起事故再次提醒我们，驾驶时必须遵守交通规则，保持安全意识，以确保自己和他人的生命安全。我们也要为那些遭受不幸的人提供支持和帮助，让他们尽快走出阴影，重新开始自己的生活。', 'llm_tag': 'None_None_ERNIE-Bot-turbo'}]}\n",
      "{'input_chats': [{'content': '1月18日，习近平总书记对政法工作作出重要指示：2014年，政法战线各项工作特别是改革工作取得新成效。新形势下，希望全国政法机关主动适应新形势，为公正司法和提高执法司法公信力提供有力制度保障。', 'role': 'user'}], 'expected_output': '孟建柱：主动适应形势新变化提高政法机关服务大局的能力', 'model_content': [{'Coherence': '6', 'Consistency': '2', 'Fluency': '2', 'Relevance': '5', 'content': '关于这个问题，您可以参阅相关内容网站，您也可以问我一些其他问题，我会尽力为您解答。', 'llm_tag': 'None_None_ERNIE-Bot-turbo'}]}\n"
     ]
    }
   ],
   "source": [
    "result_dataset = result.result_dataset\n",
    "result_df = {\"News id\":[],\"Evaluation Type\":[],\"Score\":[]}\n",
    "for i, resp in enumerate(result_dataset.list()):\n",
    "    for metric_name in evaluation_metrics.keys():\n",
    "        result_df[\"News id\"].append(i)\n",
    "        # 添加评估类型及对应得分，因为此处只用了一个大模型，所以只取第一个结果\n",
    "        result_df[\"Evaluation Type\"].append(metric_name)\n",
    "        result_df[\"Score\"].append(resp[\"model_content\"][0][metric_name])\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluation Type  Coherence  Consistency  Fluency  Relevance  Mean_Score\nNews id                                                                \n0                        8           10        3          9        7.50\n1                        8            8        2          8        6.50\n2                        6            2        2          5        3.75",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Evaluation Type</th>\n      <th>Coherence</th>\n      <th>Consistency</th>\n      <th>Fluency</th>\n      <th>Relevance</th>\n      <th>Mean_Score</th>\n    </tr>\n    <tr>\n      <th>News id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>10</td>\n      <td>3</td>\n      <td>9</td>\n      <td>7.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>8</td>\n      <td>2</td>\n      <td>8</td>\n      <td>6.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>3.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pivot_df = pd.DataFrame(result_df, index=None).pivot(\n",
    "    columns=\"Evaluation Type\", index=\"News id\", values=\"Score\"\n",
    ").astype(int)\n",
    "pivot_df['Mean_Score'] = pivot_df.mean(axis=1)\n",
    "display(pivot_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T05:31:55.090338Z",
     "start_time": "2024-01-25T05:31:55.071769Z"
    }
   },
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
