import qianfan

from evaluator import LocalEvaluator
from typing import Any, Dict, List, Union

from qianfan.evaluation.consts import (
    QianfanRefereeEvaluatorDefaultMaxScore,
    QianfanRefereeEvaluatorDefaultMetrics,
    QianfanRefereeEvaluatorDefaultSteps,
    QianfanNoRefereeEvaluatorPromptTemplate,
)
from qianfan.utils.pydantic import Field


class JudgeLocalEvaluator(LocalEvaluator):
    """local referee evaluator config class"""

    model: Any
    metric_name: str = Field(default="", description="metric name for evaluation")
    # prompt_template: Optional[Prompt] = None
    evaluation_prompt: str = Field(default=QianfanNoRefereeEvaluatorPromptTemplate)
    prompt_metrics: str = Field(default=QianfanRefereeEvaluatorDefaultMetrics)
    prompt_steps: str = Field(default=QianfanRefereeEvaluatorDefaultSteps)
    prompt_max_score: int = Field(default=QianfanRefereeEvaluatorDefaultMaxScore)

    def evaluate(
            self, input: Union[str, List[Dict[str, Any]]], reference: str, output: str
        ) -> Dict[str, Any]:
        """
        use model to evaluate in local
        :param input: given prompts.
            when is_chat in evaluateManager.eval() is true,
            input will be a chat history otherwise a prompt
        :param reference: reference answers, given by user
        :param output: output answers from llm, generated by llm

        :return: evaluate result in json schema
        """
        if isinstance(input, list):
            input_content = input[0].get("content", '')
            # 生成评价模板
            prompt = self.evaluation_prompt.format(
                criteria=self.prompt_metrics,
                steps=self.prompt_steps,
                prompt=input_content,
                response=reference,
                max_score=self.prompt_max_score,
                metric_name=self.metric_name,
            )
            # 调用模型获得评分
            msg = qianfan.Messages()
            msg.append(prompt)
            resp = self.model.do(
                messages=msg,
                temperature=0.1,
                top_p=1,
            )
            # print(f'{self.metric_name}|{input[0]}|{output}|{resp["result"].strip()}')
            return {self.metric_name: resp['result'].strip()}
        elif isinstance(input, str):
            raise ValueError(f"input in {type(input)} not supported yet")
        else:
            raise ValueError(f"input in {type(input)} not supported")